<html lang="en" dir="ltr" class="no-js lazy">

<head>
  <!--<link rel="stylesheet" href="ccs/github.css">-->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/main.css">
  <link rel="stylesheet" href="css/image-load.css">
  <link rel="stylesheet" href="css/video-load.css">
  <link rel="stylesheet" href="css/prism.css">
  <!-- <script async src="scripts/menu.js"></script> -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title></title>
</head>

<body>
<article class="container">
  <h1>Machine Learning Image Enhancement</h1>
<p>The following tweets, product of an earlier request for comments on an essay about ebooks started an interesting line of thinking.</p>
<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" data-partner="tweetdeck"><p lang="en" dir="ltr">Re: image quality, as a book publisher whose backlist is pre-digital, our problem is generally that we don’t have high-enough-res source art.</p>&mdash; Teresa Elsey (@teresaelsey) <a href="https://twitter.com/teresaelsey/status/940292077878939648?ref_src=twsrc%5Etfw">December 11, 2017</a></blockquote>
<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" data-partner="tweetdeck"><p lang="en" dir="ltr">If we&#39;re *lucky* we have a digital file for what appears in the original print book! Often we&#39;re scanning a print book to recreate the assets ...</p>&mdash; Teresa Elsey (@teresaelsey) <a href="https://twitter.com/teresaelsey/status/940303635157803008?ref_src=twsrc%5Etfw">December 11, 2017</a></blockquote>
<p>After the initial thought that poor archival practices can bite you hard I started thinking about a solution to the actual problem. You’re scanning from printed media and need a high resolution version that you can run through an image processing pipeline to create 2x and, maybe, 3x retina versions.</p>
<p>If you’ve seen the demos from Googles Machine Learning and Image processing they can do awesome things with image search and manipulation. They assume good quality images like those shot with phone cameras or some other mid to high quality cameras.</p>
<p>But what if the images are of lower quality, like those we scan from printed documents?</p>
<p>We’ll look at <a href="https://github.com/alexjc/neural-enhance">Neural Enhance</a> see if does what we need them to.</p>
<p>I’m looking to answer the following questions:</p>
<ol>
<li>Can I improve the quality of a scanned image?</li>
<li>Is the quality good enough to use in Retina displays?</li>
<li>Will the image quality remain if I run it through an image processing pipeline?</li>
</ol>
<h2>Neural Enhance</h2>
<div class="youtube-player" data-id="LhF_56SxrGk"></div>
<!-- <div class="video">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/LhF_56SxrGk?rel=0" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div> -->
<p>As seen on TV! What if you could increase the resolution of your photos using technology from CSI laboratories? Thanks to deep learning and <code>NeuralEnhance</code>, it’s now possible to train a neural network to zoom in to your images at 2x or even 4x. You’ll get even better results by increasing the number of neurons or training with a dataset similar to your low resolution image.</p>
<p>The catch? The neural network is hallucinating details based on its training from example images. It’s not reconstructing your photo exactly as it would have been if it was HD. That’s only possible in Hollywood — but using deep learning as “Creative AI” works and it is just as cool! Here’s how you can get started…</p>
<p>Rather than install all the requirements from scratch I’ll use a Docker image to run the experiment. There instructions for installing Docker in <a href="https://www.docker.com/docker-mac">Mac</a> and <a href="https://www.docker.com/docker-windows">Windows</a>. The rest of this post will assume you have installed Docker and it’s running on your system.</p>
<aside class="message warning">
  <p>The latest version of Docker for Windows requires Microsoft Windows 10 Professional or Enterprise 64-bit.</p>
</aside>
<p>Download the Docker image using the <code>docker</code> CLI:</p>
<pre><code class="language-bash"># Download the Docker image and show the help text to
# make sure it works.
docker run --rm -v `pwd`:/ne/input -it alexjc/neural-enhance --help
</code></pre>
<p>For Macintosh and Linux users add the following command to your <code>aliases</code> or <code>.bashrc</code>. This will allow users to run the command as <code>enhance</code> without a <code>.py</code> extension.</p>
<p>As the command indicates this should be written as a single line.</p>
<pre><code class="language-bash"># All three lines below should be in one line. I've inserted hard returns for readability
alias enhance='function ne() { docker run --rm -v \
&quot;$(pwd)/`dirname ${@:$#}`&quot;:/ne/input -it alexjc/neural-enhance $ \
{@:1:$#-1} &quot;input/`basename ${@:$#}`&quot;; }; ne'
</code></pre>
<p>The block below shows how to run <code>enhance</code>. The first example repairs the artifacts in an image keeping the resolution of the original.</p>
<p>The second example will create 2x images for all the <code>jpg</code> images inside the images folder.</p>
<pre><code class="language-bash">enhance --zoom=1 --model=repair images/broken.jpg
# Process multiple images, make sure to quote the argument!
enhance --zoom=2 &quot;images/*.jpg&quot;
</code></pre>
<p>For this example we’ll use an old black and white image I downloaded from an old article about <a href="https://www.wikiwand.com/en/SIMNET">Simnet</a>. Figure one shows the original image, without retouch.</p>
<figure>
  <img src='http://publishing-project.rivendellweb.net/wp-content/uploads/2017/12/simnet-concept-2.jpg' alt='Simnet Concept Base Image'>
  <figcaption>Simnet Concept Base Image</figcaption>
</figure>
<pre><code class="language-bash"># Run the super-resolution script to repair JPEG artefacts, zoom factor 1:1.
enhance --type=photo --model=repair --zoom=1 simnet-concept.jpg
</code></pre>
<p>Figure two shows the image with JPEG artifacts repaired but still at the same resolution as the original.</p>
<figure>
  <img src='http://publishing-project.rivendellweb.net/wp-content/uploads/2017/12/simnet-concept_ne1x.png'
    alt='Simnet Concept Image Repaired at 1x Resolution'>
  <figcaption>Simnet Concept Image Repaired at 1x Resolution</figcaption>
</figure>
<p>Figure 3 shows the image at twice the size of the original. It has eliminated the artifacts of the original (if it had any) and increased the size so it’s now usable in a 2x Retina display.</p>
<pre><code class="language-bash"># Process multiple good quality images with a single run, zoom factor 2:1.
enhance.py --type=photo --zoom=2 simnet-concept.jpg
</code></pre>
<figure>
  <img src='http://publishing-project.rivendellweb.net/wp-content/uploads/2017/12/simnet-concept_ne2x.png' alt='Simnet Concept Image Repaired at 1x Resolution'>
  <figcaption>Simnet Concept Image at 2x Resolution</figcaption>
</figure>
<h2>Training your own models</h2>
<p>The models bundled with Neural Enhance will only work with 1:1 and 2x resolutions. Anyting bigger will require to train your own model</p>
<p>If working with scanned images it may become necessary to create a new model for each book based on the scanned images. Some examples of commands to train models.</p>
<p>First we remove the model file. Don’t want to reload the data to fine-tune it.</p>
<pre><code class="language-bash">rm -f ne?x*.pkl.bz2
</code></pre>
<p>Pre-train the model using perceptual loss and algorithm that uses  <strong>convolutional neural networks</strong> using a per-pixel loss and  <strong>perceptual loss functions</strong> based on high-level features extracted from pretrained networks.</p>
<pre><code class="language-bash">enhance --train &quot;data/*.jpg&quot; --model custom \
  --scales=2 --epochs=50 \
  --perceptual-layer=conv2_2 --smoothness-weight=1e7 \
  --adversary-weight=0.0 --generator-blocks=4 \
  --generator-filters=64
</code></pre>
<p>Train the model using an <a href="https://www.wikiwand.com/en/Generative_adversarial_network">Generative adversarial network</a>.</p>
<pre><code class="language-bash">enhance --train &quot;data/*.jpg&quot; --model custom \
  --scales=2 --epochs=250 \
  --perceptual-layer=conv5_2 --smoothness-weight=2e4 \
  --adversary-weight=1e3 \
  --generator-start=5 --discriminator-start=0 \
  --adversarial-start=5 --discriminator-size=64
</code></pre>
<p>Additional models can be trained by playing with the different parameter available to the <code>enhance</code> tool. We can get a list of the parameters by running the same command we used to install and check the image:</p>
<pre><code class="language-bash"># Download the Docker image and show the help text to
# make sure it works.
docker run --rm -v `pwd`:/ne/input -it alexjc/neural-enhance --help
</code></pre>
<p>The results look promising but to answer the questions I asked originally:</p>
<ol>
<li><strong>Can I improve the quality of a scanned image?</strong> I can fix artifacts in the low resolution image that will improve the quality but it’s done through interpolation so the quality may not improve as much as you think it does</li>
<li><strong>Is the quality good enough to use in Retina displays?</strong> Using the zoom parameter with a value of 2, it generates an image with the required number of pixels. If you want to work with higher image densities you’ll have to train your own model</li>
<li><strong>Will the image quality remain if I run it through an image processing pipeline?</strong> Unless we’re generating a very large version of the image, we don’t have enough pixels beyond 2x retina. We can train the model interpolate to higher resolutions but the result is far from guaranteed</li>
</ol>
<h2>Links and Resources</h2>
<ul>
<li>Tools and Libraries
<ul>
<li>Neural Enhance
<ul>
<li><a href="https://github.com/alexjc/neural-enhance">Github</a></li>
</ul>
</li>
<li>Raisr
<ul>
<li><a href="https://www.theverge.com/2016/11/16/13649016/google-machine-learning-low-res-image-raisr">Google’s prototype machine learning software lets you enhance low-res photos</a></li>
<li><a href="https://arxiv.org/pdf/1606.01299.pdf">RAISR: Rapid and Accurate Image Super Resolution</a></li>
<li><a href="https://drive.google.com/file/d/0BzCe024Ewz8ab2RKUFVFZGJ4OWc/view">Supplementary Material for RAISR: Rapid and Accurate Image Super Resolution</a></li>
<li><a href="https://github.com/volvet/RAISR-1">Unofficial Python implementation of RAISR</a></li>
</ul>
</li>
<li><a href="https://arstechnica.com/information-technology/2017/02/google-brain-super-resolution-zoom-enhance/">Google Brain super-resolution image tech makes “zoom, enhance!” real</a></li>
<li><a href="http://mashable.com/2017/11/03/lets-enhance-photo-ai/#mjbgDIU8Mgqs">Website uses neural networks to enlarge small images, and the results are pretty magical</a>
<ul>
<li><a href="https://letsenhance.io/">Let’s Enhance</a></li>
</ul>
</li>
<li><a href="https://petapixel.com/2017/11/01/photo-enhancement-starting-get-crazy/">Photo Enhancement is Starting to Get Crazy</a>
<ul>
<li><a href="http://webdav.tuebingen.mpg.de/pixel/enhancenet/">EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</a></li>
</ul>
</li>
<li>PRSR
<ul>
<li><a href="https://arxiv.org/pdf/1702.00783v2.pdf">Pixel Recursive Super Resolution</a></li>
<li><a href="https://github.com/nilboy/pixel-recursive-super-resolution">Github Repository</a></li>
</ul>
</li>
</ul>
</li>
<li>Research Background
<ul>
<li><a href="https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li>
<li><a href="https://arxiv.org/abs/1609.05158">Real-Time Super-Resolution Using Efficient Sub-Pixel Convolution</a></li>
<li><a href="https://arxiv.org/abs/1511.04491">Deeply-Recursive Convolutional Network for Image Super-Resolution</a></li>
<li><a href="https://arxiv.org/abs/1609.04802">Photo-Realistic Super-Resolution Using a Generative Adversarial Network</a></li>
</ul>
</li>
</ul>

</article>
<!--
</div> -->
<script src="scripts/lazy-load.js"></script>
<script src="scripts/vendor/clipboard.min.js"></script>
<script src="scripts/vendor/prism.js"></script>
<script src="scripts/vendor/fontfaceobserver.js"></script>
<script src="scripts/load-fonts.js"></script>
<script src="scripts/lazy-load-video.js"></script>
</body>
</html>