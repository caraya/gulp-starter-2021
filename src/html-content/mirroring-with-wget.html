<h1>Mirroring a site using Wget</h1>
<p><a href="https://www.gnu.org/software/wget/">Wget</a> is the GNU/FSF alternative to <a href="https://curl.haxx.se/">CURL</a> used to retrieve files from the network via command line. Gettting a single file is easy but trying to get an entire directory it’s not so easy.</p>
<p>I’ve used a similar version to this command in the past but it turned into huge downloads that would take forever to complete.</p>
<p>I found this version in <a href="https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/">Guy Rutenberg’s site</a> and liked it after I used it to mirror a site.</p>
<p>The command is:</p>
<pre><code class="language-bash">wget --mirror \
--convert-links \
--adjust-extension \
--page-requisites
--no-parent http://example.org
</code></pre>
<p>These are flags we’re using:</p>
<ul>
<li>
<p><strong>–mirror</strong> – Mirrors a site by making the download recursive and enabling additional flags</p>
</li>
<li>
<p><strong>–convert-links</strong> – After the download is complete, convert the links in the document to make them suitable for local viewing. This affects not only the visible hyperlinks, but any part of the document that links to external content</p>
</li>
<li>
<p><strong>–adjust-extension</strong> – If a file of type <code>application/xhtml+xml</code> or <code>text/html</code> is downloaded and the URL does not end with the regexp <code>\.[Hh][Tt][Mm][Ll]?</code>, this option will cause the suffix ‘.html’ to be appended to the local filename.</p>
<p>This is useful, for instance, when you’re mirroring a remote site that uses ‘.asp’ pages, but you want the mirrored pages to be viewable on your stock Apache server</p>
</li>
<li>
<p><strong>–page-requisites</strong> – Download things like CSS style-sheets and images required to properly display the page offline.</p>
</li>
<li>
<p><strong>–no-parent</strong> – Do not go to the parent directory. It useful for restricting the download to only a portion of the site.</p>
</li>
</ul>
<p>Alternatively, the command above may be shortened:</p>
<pre><code class="language-bash">wget -mkEpnp http://example.org
</code></pre>
<p>If the server’s robots.txt file is not configured to forbid it, you can run this command to mirror the specified directory and prepare it for offlline viewing by making some changes to the content and downloading all the needed materials to make the page viewable  offlline or in a different server.</p>
